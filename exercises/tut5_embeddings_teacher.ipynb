{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/adams/blob/master/exercises/tut5_embeddings_teacher.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 5: Word embeddings and Word2Vec\n",
    "The tutorial covers word embeddings in general and one of the most well-known models in this matter, the so-called Word-to-Vec (W2V). For this purpose, we reconsider the text dataset IMDB from the last tutorial. Yet, this time we preprocess the data with `Keras` using the `TextVectorization` layer that facilitates the standardization, tokenization and indexing. Next, we create a simple binary classification model using word embeddings to grasp the essence in practice. Finally, we apply the W2V model to the same data using [Gensim library](https://pypi.org/project/gensim/). \n",
    "\n",
    "Several libraries make things easier if the aim is to use W2V directly. The [Gensim library](https://pypi.org/project/gensim/) is one of them that offers a friendly interface to train embeddings, as you will see in this tutorial. \n",
    "\n",
    "However, if you would like to start from scratch and code W2V yourself using just `NumPy`, we recommend [Nathan Rooy's post](https://nathanrooy.github.io/posts/2018-03-22/word2vec-from-scratch-with-python-and-numpy/). Or, if you would like to do it with `TensorFlow`, there is an excellent tutorial [here](https://www.tensorflow.org/tutorials/text/word2vec) from the TensorFlow website.\n",
    "\n",
    "The outline then is the following\n",
    "1. Preparing the IMDB dataset with `Keras`\n",
    "2. Understanding embeddings with a simple binary classification model\n",
    "3. Word2Vec using `Gensim`\n",
    "\n",
    "For further examples, please visit the demo [word-2-vec.ipynb](https://github.com/Humboldt-WI/adams/blob/master/demos/nlp/word-2-vec.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preparing the IMDB dataset\n",
    "Setting things up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import io\n",
    "import re\n",
    "import string\n",
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import Word2Vec  \n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Load the `IMDB-50K-Movie-Review.zip` file, and map the labels to 1 (positive) and 0 (negative). Then, have a look at the first rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review  sentiment\n",
      "0  One of the other reviewers has mentioned that ...          1\n",
      "1  A wonderful little production. <br /><br />The...          1\n",
      "2  I thought this was a wonderful way to spend ti...          1\n",
      "3  Basically there's a family where a little boy ...          0\n",
      "4  Petter Mattei's \"Love in the Time of Money\" is...          1\n",
      "(50000, 2)\n"
     ]
    }
   ],
   "source": [
    "# load the data (be sure to provide the correct file path)\n",
    "total_imbd = pd.read_csv(\"IMDB-50K-Movie-Review.zip\", sep=\",\", encoding=\"ISO-8859-1\")\n",
    "total_imbd['sentiment'] = total_imbd['sentiment'].map({'positive' : 1, 'negative': 0})\n",
    "print(total_imbd.head())\n",
    "print(total_imbd.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Split the data into training and validation sets. Use 80% of the data for training. You can use `train_test_split()` function from `sklearn`. In addition, transform the sets into `NumPy` arrays using `to_numpy()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data\n",
    "X_train, X_val, y_train, y_val = train_test_split(total_imbd['review'], total_imbd['sentiment'], test_size = 0.2, random_state = 5)\n",
    "# transform them to numpy\n",
    "X_train = X_train.to_numpy()\n",
    "X_val = X_val.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_val = y_val.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far, we have just created training and validation sets of text and labels. However, we cannot feed a neural network with this text format. We need numeric tensors. \n",
    "\n",
    "The transformation of text to numeric tensors is known as *vectorization*. This process can be split into three steps:\n",
    "1. **Standardization** of the text, such as removing punctuation, converting all the text to lowercase, etc.\n",
    "2. **Tokeinzation** of the standardized text, where we separate the text into units or *tokens*, usually words or n-grams.\n",
    "3. **Indexing** of the tokens into a numerical vector.\n",
    "\n",
    "These 3 steps are implemented in the Keras `TextVectorization` layer.\n",
    "\n",
    "```python\n",
    "TextVectorization(\n",
    "    standardize = our_standardization,\n",
    "    max_tokens = vocab_size,\n",
    "    output_sequence_length = seq_length\n",
    "    )\n",
    "```\n",
    "Where `our_standardization` is our customized function to standardize the text, for example, we saw that some examples in the dataset have HTML tags `<br />`, and we'd like to delete them. \n",
    "\n",
    "### Exercise 3\n",
    "So, let's first build our own standardization function called `our_standardization`. The function should convert uppercase to lowercase (`tf.strings.lower`), remove HTML tags (`tf.strings.regex_replace`), deletes the punctuation (`re.escape(string.punctuation)`) and double spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def our_standardization(text_data):\n",
    "  lowercase = tf.strings.lower(text_data) # convert to lowercase\n",
    "\n",
    "  remove_html = tf.strings.regex_replace(lowercase, '<br />', ' ') # remove HTML tags, alternatively remove html characters\n",
    "  #in advance with beautifulsoup.\n",
    "  \n",
    "  pattern_remove_punctuation = '[%s]' % re.escape(string.punctuation) # pattern to remove punctuation\n",
    "  \n",
    "  remove_punct = tf.strings.regex_replace(remove_html, pattern_remove_punctuation, '') # apply pattern\n",
    "    \n",
    "  remove_double_spaces = tf.strings.regex_replace(remove_punct, '\\s+', ' ') # remove double space\n",
    "  return remove_double_spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\[\\\\\\\\\\\\]\\\\^_`\\\\{\\\\|\\\\}\\\\~'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.escape(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[!\"\\\\#\\\\$%\\\\&\\'\\\\(\\\\)\\\\*\\\\+,\\\\-\\\\./:;<=>\\\\?@\\\\[\\\\\\\\\\\\]\\\\^_`\\\\{\\\\|\\\\}\\\\~]'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'[%s]' % re.escape(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[example]'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'[%s]' % 'example'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Apply the `our_standardization` function to the following text and see how it works\n",
    "\n",
    "`\"Bruce Dern also is in the mix and Dern never fails to fascinate in about any film.<br /><br />The movie could be considered kind of downer to the average viewer, but I found it fascinating....and I don't like depressing movies normally. What I found was a kind of quirky crime film.\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=string, numpy=b'bruce dern also is in the mix and dern never fails to fascinate in about any film the movie could be considered kind of downer to the average viewer but i found it fascinatingand i dont like depressing movies normally what i found was a kind of quirky crime film'>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# An example of the our_standardization function\n",
    "our_standardization(\"Bruce Dern also is in the mix and Dern never fails to fascinate in about any film.<br /><br />The movie could be considered kind of downer to the average viewer, but I found it fascinating....and I don't like depressing movies normally. What I found was a kind of quirky crime film.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "Great! So let's now vectorize our data (use `TextVectorization`) with a vocabulary of the first 10,000 most frequent words and a maximum sequence of the text of 100 characters. Called this layer `vectorize_layer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the vocabulary and the max number of words in a sequence\n",
    "vocab_size = 10000\n",
    "seq_length = 100\n",
    "\n",
    "# Create a vectorization layer\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize = our_standardization,\n",
    "    max_tokens = vocab_size,\n",
    "    output_sequence_length = seq_length\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "Index the vocabulary. To do it, you need to call the `adapt()` method from the `vectorize_layer` and apply it to `X_train`. Then, retrieve the computed vocabulary using `get_vocabulary()` and save it into `vocab`. Finally, print the first 10 words of the vocabulary (`print(vocab[:10])`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['', '[UNK]', 'the', 'a', 'and', 'of', 'to', 'is', 'in', 'it']\n"
     ]
    }
   ],
   "source": [
    "# To create the vocabulary, we need to call adapt. The input is only the text\n",
    "vectorize_layer.adapt(X_train)\n",
    "# Check the first 10 words of the vocabulary. It is sorted by frequency \n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "print(vocab[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "Apply the `vectorization_layer` to the same previous example, i.e. To\n",
    "\n",
    "`\"Bruce Dern also is in the mix and Dern never fails to fascinate in about any film.<br /><br />The movie could be considered kind of downer to the average viewer, but I found it fascinating....and I don't like depressing movies normally. What I found was a kind of quirky crime film.\"`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 100), dtype=int64, numpy=\n",
       "array([[1389, 8562,   82,    7,    8,    2, 1579,    4, 8562,  109,  976,\n",
       "           6,    1,    8,   42,   99,   19,    2,   17,   97,   26, 1206,\n",
       "         236,    5,    1,    6,    2,  871,  527,   18,   10,  245,    9,\n",
       "           1,   10,   89,   38, 2234,   92, 1805,   48,   10,  245,   13,\n",
       "           3,  236,    5, 2653,  832,   19,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0]], dtype=int64)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the vectorization layer\n",
    "vectorize_layer([\"Bruce Dern also is in the mix and Dern never fails to fascinate in about any film.<br /><br />The movie could be considered kind of downer to the average viewer, but I found it fascinating....and I don't like depressing movies normally. What I found was a kind of quirky crime film.\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To better understand what is happening in the TextVectorization layer we can use less data, smaller vocab_size and smaller number of max_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vectorization layer\n",
    "vectorize_layer_smaller = TextVectorization(\n",
    "    standardize = our_standardization,\n",
    "    max_tokens = 10,\n",
    "    output_sequence_length = 20\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X_tr=np.array([[\"Bruce Dern also is in the mix and Dern never fails to fascinate in about\"],\n",
    "              [\"Bruce Dern also \"],\n",
    "              [\"Bruce Dern also is in the mix\"],\n",
    "              [\"Bruce Dern also is in the mixdasfdsfds\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'dern', 'in', 'bruce', 'also', 'the', 'is', 'mix', 'to']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorize_layer_smaller.adapt(X_tr)\n",
    "# Check the first 10 words of the vocabulary. It is sorted by frequency \n",
    "vocab_smaller = vectorize_layer_smaller.get_vocabulary()\n",
    "vocab_smaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr_vec=vectorize_layer_smaller(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 20), dtype=int64, numpy=\n",
       "array([[4, 2, 5, 7, 3, 6, 8, 1, 2, 1, 1, 9, 1, 3, 1, 0, 0, 0, 0, 0],\n",
       "       [4, 2, 5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [4, 2, 5, 7, 3, 6, 8, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [4, 2, 5, 7, 3, 6, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
       "      dtype=int64)>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tr_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bruce', 'dern', 'also', 'is', 'in', 'the', 'mix', '[UNK]', 'dern', '[UNK]', '[UNK]', 'to', '[UNK]', 'in', '[UNK]', '', '', '', '', ''] \n",
      " ['bruce', 'dern', 'also', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', ''] \n",
      " ['bruce', 'dern', 'also', 'is', 'in', 'the', 'mix', '', '', '', '', '', '', '', '', '', '', '', '', ''] \n",
      " ['bruce', 'dern', 'also', 'is', 'in', 'the', '[UNK]', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
     ]
    }
   ],
   "source": [
    "X_tr_words = [[vocab_smaller[w] for w in rev] for rev in X_tr_vec]\n",
    "print(X_tr_words[0],'\\n',X_tr_words[1],'\\n',X_tr_words[2],'\\n',X_tr_words[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Understanding word embeddings\n",
    "We have not introduced any embedding layer until now; instead, we created a vectorization layer that can transform text inputs into numeric tensors. So, for example, the text \"tomorrow is Saturday\" will be transformed into something like `[23, 45, 5, 0, 0, 0]` (if the length of the sequence is 6). An embedding layer transforms each word, which can be thought of as a one-hot vector, into another more dense vector. \n",
    "\n",
    "Let's see an example of the `Embedding` layer in Keras, where we hypothetically have only 100 words in the vocabulary, and we want to transform this space into a 5-dimensional one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result1: (6, 5) \n",
      "result2: (2, 6, 5)\n"
     ]
    }
   ],
   "source": [
    "# Create the embedding layer of shape (100,5)\n",
    "embedding_layer = layers.Embedding(100,#input dimension: size of the vocabulary\n",
    "                                   5)#output dimension: size of dense embedding\n",
    "# Feed a sequence of word indices\n",
    "result1 = embedding_layer(tf.constant([23, 45, 5, 0, 0, 0]))\n",
    "# We can also feed batches    \n",
    "result2 = embedding_layer(tf.constant([[23, 45, 5, 0, 0, 0],\n",
    "                                       [3, 4, 55, 4, 0, 0]]))\n",
    "print(\"result1:\",result1.shape,\"\\nresult2:\",result2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each word index has been transformed into a 5-dimensional vector (in this case, random values). For example, the values of `results1` are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(6, 5), dtype=float32, numpy=\n",
       "array([[-0.0195032 , -0.04639771, -0.00647088, -0.03572767, -0.00363492],\n",
       "       [ 0.04764559, -0.02657135,  0.03978487,  0.01372594,  0.00591636],\n",
       "       [-0.02045833, -0.02105316,  0.03799187,  0.02469781, -0.02553259],\n",
       "       [ 0.01683796,  0.03149254, -0.03696914,  0.04902716,  0.03255706],\n",
       "       [ 0.01683796,  0.03149254, -0.03696914,  0.04902716,  0.03255706],\n",
       "       [ 0.01683796,  0.03149254, -0.03696914,  0.04902716,  0.03255706]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "array([-0.0195032 , -0.04639771, -0.00647088, -0.03572767, -0.00363492],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.trainable_variables[0][23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5,), dtype=float32, numpy=\n",
       "array([ 0.04764559, -0.02657135,  0.03978487,  0.01372594,  0.00591636],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_layer.trainable_variables[0][45]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding layer: dictionary which maps integer indices of inputs to dense vectors (embedding weights). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 8\n",
    "Build a simple model to infer the sentiment. To do it, use a `Sequential` model where the first layer transforms the text into tensors (`vectorize_layer`), the second layer embeds the vocabulary into a 16-dimension (`layers.Embedding`), the third layer uses `layers.GlobalAveragePooling1D()` to reduce the complete text to a single average vector in the embedding space, and finally, use a `Dense` layer with a sigmoid activation to infer the sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a simple model to use word embeddings\n",
    "embedding_dim = 16\n",
    "\n",
    "model = Sequential([\n",
    "  vectorize_layer,\n",
    "  layers.Embedding(vocab_size, embedding_dim, name=\"embedding\"), \n",
    "  layers.GlobalAveragePooling1D(), # each sample is reduced to the average of the word embeddings\n",
    "  layers.Dense(1, activation='sigmoid')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 9 \n",
    "Compile the model with the `rmsprop` optimizer, the adequate loss function and monitor the `accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's compile it\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy', # positive or negative\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 10\n",
    "Fit the model using 10 `epochs`. Remember to specify the validation dataset in `validation_data`. How accurate is the model in the validation set? How many parameters does it have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.6639 - accuracy: 0.6449 - val_loss: 0.6153 - val_accuracy: 0.7078\n",
      "Epoch 2/10\n",
      "1250/1250 [==============================] - 9s 7ms/step - loss: 0.5547 - accuracy: 0.7416 - val_loss: 0.5065 - val_accuracy: 0.7661\n",
      "Epoch 3/10\n",
      "1250/1250 [==============================] - 8s 7ms/step - loss: 0.4643 - accuracy: 0.7896 - val_loss: 0.4463 - val_accuracy: 0.7940\n",
      "Epoch 4/10\n",
      "1250/1250 [==============================] - 8s 7ms/step - loss: 0.4142 - accuracy: 0.8128 - val_loss: 0.4147 - val_accuracy: 0.8099\n",
      "Epoch 5/10\n",
      "1250/1250 [==============================] - 8s 7ms/step - loss: 0.3834 - accuracy: 0.8306 - val_loss: 0.3958 - val_accuracy: 0.8187\n",
      "Epoch 6/10\n",
      "1250/1250 [==============================] - 8s 7ms/step - loss: 0.3622 - accuracy: 0.8406 - val_loss: 0.3838 - val_accuracy: 0.8256\n",
      "Epoch 7/10\n",
      "1250/1250 [==============================] - 14s 12ms/step - loss: 0.3466 - accuracy: 0.8489 - val_loss: 0.3766 - val_accuracy: 0.8297\n",
      "Epoch 8/10\n",
      "1250/1250 [==============================] - 18s 14ms/step - loss: 0.3343 - accuracy: 0.8554 - val_loss: 0.3746 - val_accuracy: 0.8304\n",
      "Epoch 9/10\n",
      "1250/1250 [==============================] - 14s 11ms/step - loss: 0.3244 - accuracy: 0.8612 - val_loss: 0.3710 - val_accuracy: 0.8337\n",
      "Epoch 10/10\n",
      "1250/1250 [==============================] - 13s 11ms/step - loss: 0.3163 - accuracy: 0.8659 - val_loss: 0.3713 - val_accuracy: 0.8349\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29fe8946d00>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ~ 2 minutes\n",
    "model.fit(\n",
    "    X_train,\n",
    "    y_train,\n",
    "    validation_data = (X_val, y_val),\n",
    "    epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " text_vectorization_1 (TextV  (None, 100)              0         \n",
      " ectorization)                                                   \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, 100, 16)           160000    \n",
      "                                                                 \n",
      " global_average_pooling1d (G  (None, 16)               0         \n",
      " lobalAveragePooling1D)                                          \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 160,017\n",
      "Trainable params: 160,017\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Check the number of trainable parameters\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer:  <keras.layers.preprocessing.text_vectorization.TextVectorization object at 0x0000029FE84D8970>\n",
      "Output shape:  (None, 100)\n",
      "Number of weight matrices of trainable variables:  0\n",
      "\n",
      "\n",
      "Layer:  <keras.layers.core.embedding.Embedding object at 0x0000029FE84D86A0>\n",
      "Output shape:  (None, 100, 16)\n",
      "Number of weight matrices of trainable variables:  1\n",
      "Trainable Weight matrix  1 :  (10000, 16)\n",
      "\n",
      "\n",
      "Layer:  <keras.layers.pooling.global_average_pooling1d.GlobalAveragePooling1D object at 0x0000029FE84D8490>\n",
      "Output shape:  (None, 16)\n",
      "Number of weight matrices of trainable variables:  0\n",
      "\n",
      "\n",
      "Layer:  <keras.layers.core.dense.Dense object at 0x0000029FE88FB040>\n",
      "Output shape:  (None, 1)\n",
      "Number of weight matrices of trainable variables:  2\n",
      "Trainable Weight matrix  1 :  (16, 1)\n",
      "Trainable Weight matrix  2 :  (1,)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for lr in model.layers:\n",
    "    print('Layer: ',lr)\n",
    "    nr_tr_vars=len(lr.trainable_variables)\n",
    "    print('Output shape: ',lr.output_shape)\n",
    "    print('Number of weight matrices of trainable variables: ',nr_tr_vars)\n",
    "    if nr_tr_vars>0:\n",
    "        for nr_tr in range(0,nr_tr_vars):\n",
    "            print('Trainable Weight matrix ',nr_tr+1,': ',lr.trainable_variables[nr_tr].shape)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec with Gensim library\n",
    "Let's now apply W2V to the same text data. Remember that W2V proposes two models for learning word vectors, continuous-bag-of-words (CBOW) and Skip-Gram (SG). In a nutshell, CBOW predicts a central <font color='yellow'>target</font> word from surrounding <font color='green'>context</font> words, while SG takes the opposite approach. Given a <font color='yellow'>target</font> word, predict <font color='green'>context</font> words. For example, using a window size of 2 to the following phrase\n",
    "\n",
    "> I finally <font color='green'>found a</font><font color='yellow'> machine</font><font color='green'> at the </font>  gym that I like: the vending machine!\n",
    "\n",
    "So in CBOW, the problem is \n",
    "\n",
    "[I finally <font color='green'>found a</font><font color='yellow'> ?</font><font color='green'> at the </font> gym]\n",
    "\n",
    "And in SG,\n",
    "\n",
    "[I finally <font color='green'>? ?</font><font color='yellow'> machine</font><font color='green'> ? ? </font> gym]\n",
    "\n",
    "In this tutorial, we apply SG (argument `sg=1` in `gensim`). But you are welcome to compare results for CBOW (`sg=0`).\n",
    "\n",
    "### Exercise 11\n",
    "Create `X_train_vec` by applying the vectorization layer you have already created to `X_train`. In this way, we will be using the same vectorization procedure as before (same vocabulary, length of the sequence, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply vectorize_layer to X_train\n",
    "X_train_vec = vectorize_layer(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(100,), dtype=int64, numpy=\n",
       "array([  46,    7,   33,  312,  278, 7117,  403,  789,    6,  388,    9,\n",
       "         57,   25, 7288,    8, 2121,   15,    3,    1,    1,    2,   17,\n",
       "       4666, 1092,   98,  202,    4,   13,    1,  339,   40,    3, 3421,\n",
       "        734, 9110, 1176,    6, 3517,    4, 5112,   36,  514,   67,    3,\n",
       "       3593,  106, 2160,  155,   63,  355,   94,    2,   80,  410,   23,\n",
       "         14, 1285,    4,  221,   14,  777, 2864,   14, 3624,    1,  110,\n",
       "         10,  742,    6, 4417,    2,  219,   18,    2, 5407,   27, 2612,\n",
       "         12, 1653,    7,   24,    1,    2,  123,    2,   17,  208,   31,\n",
       "          1,  649,   10,   38,    2,  187,   12,    1,  594, 2654,   13,\n",
       "        343], dtype=int64)>"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_vec[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 12\n",
    "Gensim accepts words, so convert the `X_train_vec` into a list of words. Called this object `X_train_words`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ 6 min\n",
    "X_train_words = [[vocab[w] for w in rev] for rev in X_train_vec]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 13\n",
    "Train a W2V model using `Word2Vec` function and `X_train_words` as input (call it `w2v_model`). Use `min_count` of 1, a `window` of 5, 50 `epochs`, a `vector_size` of 100 for the embeddings and SG (`sg=1`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=10000, vector_size=100, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# Train a Word2Vec model ~ 5 min\n",
    "w2v_model = Word2Vec(X_train_words, #X_train_vec.numpy(), \n",
    "                 min_count=1,  #min_count means the frequency benchmark\n",
    "                 window=5,     #the size of context\n",
    "                 epochs=50,  \n",
    "                 vector_size=100, #size of embedding\n",
    "                 workers=4,#for parallel computing\n",
    "                 sg  = 1)    \n",
    "# summarize the loaded model\n",
    "print(w2v_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.7422866 ,  0.07011281,  0.17215607, -0.06441133, -0.18116634,\n",
       "       -0.3497524 , -0.1832129 ,  0.19918127,  0.05572874, -0.35483485,\n",
       "        0.0619622 , -0.19628175,  0.24171843, -0.23548499,  0.28379336,\n",
       "        0.21866822, -0.00950472, -0.08436187,  0.23246624, -0.37449208,\n",
       "        0.02789638,  0.06735098,  0.30410182,  0.15329996, -0.24441405,\n",
       "        0.2002234 , -0.01896348,  0.1310914 , -0.08164166,  0.2830613 ,\n",
       "       -0.07862464, -0.11080905, -0.01053677,  0.17467418,  0.01008057,\n",
       "        0.20077197,  0.14305808, -0.06565698, -0.30597922,  0.13186269,\n",
       "        0.1908682 , -0.02755645, -0.05216184,  0.3922729 , -0.09934577,\n",
       "        0.21803425, -0.06046042, -0.19633771, -0.13130325,  0.37006807,\n",
       "       -0.1257119 , -0.19493343,  0.17493409, -0.20511326, -0.13973004,\n",
       "        0.30518663, -0.15295902, -0.12946807,  0.22887555,  0.3490611 ,\n",
       "       -0.13998403,  0.08421545,  0.20027298,  0.2395708 , -0.08650368,\n",
       "        0.10091771,  0.21836822,  0.1443163 , -0.11423711,  0.15822   ,\n",
       "       -0.37259167, -0.03925966,  0.43947738,  0.1615423 ,  0.0738994 ,\n",
       "       -0.02866086, -0.3433443 , -0.27139625,  0.06206842, -0.11583203,\n",
       "       -0.0077243 , -0.2868803 ,  0.18896285, -0.14729306, -0.45711288,\n",
       "        0.1906634 , -0.05884508, -0.09263992, -0.2879864 ,  0.0981258 ,\n",
       "        0.23395954,  0.20765442,  0.07781348,  0.04272557,  0.23118787,\n",
       "       -0.1534734 , -0.06490111,  0.02252833,  0.2576596 ,  0.21172436],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv['great']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words below the min_count frequency are dropped before training occurs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 14\n",
    "Check how similar are great to good, great to horrible and so on. Use `w2v_model.wv.similarity()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.71228135\n",
      "0.3995029\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.wv.similarity('great', 'good'))\n",
    "print(w2v_model.wv.similarity('great', 'horrible'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 15\n",
    "Get the `topn` 5 most similar words to great. Use `w2v_model.wv.most_similar` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wonderful', 0.8117287158966064),\n",
       " ('fantastic', 0.7801386117935181),\n",
       " ('fine', 0.7010308504104614),\n",
       " ('good', 0.6951696872711182),\n",
       " ('terrific', 0.6847767233848572)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model.wv.most_similar('great', topn = 5)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "071737a5efb5187f1b8a7f5eacd9bb694a30cbbaa4393dd0a3bebb490d9d36dd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
