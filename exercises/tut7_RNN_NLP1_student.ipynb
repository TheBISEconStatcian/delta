{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Humboldt-WI/adams/blob/master/exercises/tut7_RNN_NLP1_student.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial 7: Processing words as sequences\n",
    "In this tutorial, we will try to predict the next word in a sentence. This is challenging, as we will see because we choose a word out of a vocabulary, which is commonly large. Hence, the purpose of this tutorial is not to get an accurate model, but rather to show you how this task can be performed. More accurate models require larger samples and computational resources. \n",
    "\n",
    "We cover the following\n",
    "1. Prepare the text data to represent the sequence $[w_1,w_2,w_3,w_4,w_5,w_6]$ into something like $y=w_6$ and $x=[w_5,w_4,w_3,w_2,w_1]$. Because you are now familiar with IMBD dataset, we will use it to create our sequence data.\n",
    "2. Train a feedforward network. \n",
    "3. Train a NN with `SimpleRNN` layer. \n",
    "4. Train a NN with `LSTM` layer.\n",
    "5. Train a NN with `Embedding` and `LSTM` layers.\n",
    "\n",
    "For further examples, please visit the demos in [demos/rnn](https://github.com/Humboldt-WI/adams/tree/master/demos/rnn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocess IMDB data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import string\n",
    "import re\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import TextVectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "Load the IMDB, and use the first 100 reviews as training and the next 20 as validation. We won't be using the sentiment, only the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data (be sure to provide the correct file path)\n",
    "data = pd.read_csv(\"IMDB-50K-Movie-Review.zip\", sep=\",\", encoding=\"ISO-8859-1\")\n",
    "begin_training_at = 0\n",
    "train_size = 100\n",
    "train = data['review'][begin_training_at:train_size + begin_training_at]\n",
    "val_size = 20\n",
    "val = data['review'][begin_training_at + train_size:begin_training_at + train_size + val_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Create `our_standardization` function to convert to lowercase, remove HTML tags, punctation and double spaces (check [tut5_embeddings](https://github.com/Humboldt-WI/adams/blob/master/exercises/tut5_embeddings_teacher.ipynb)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def our_standardization(text_data):\n",
    "def our_standardization(text_data):\n",
    "  lowercase = tf.strings.lower(text_data) # convert to lowercase\n",
    "\n",
    "  remove_html = tf.strings.regex_replace(lowercase, '<.*?>', ' ') # remove HTML tags, alternatively remove html characters\n",
    "  #in advance with beautifulsoup.\n",
    "  \n",
    "  pattern_remove_punctuation = '[%s]' % re.escape(string.punctuation) # pattern to remove punctuation\n",
    "  \n",
    "  remove_punct = tf.strings.regex_replace(remove_html, pattern_remove_punctuation, '') # apply pattern\n",
    "    \n",
    "  remove_double_spaces = tf.strings.regex_replace(remove_punct, '\\s+', ' ') # remove double space\n",
    "  return remove_double_spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3\n",
    "Create `TextVectorization` with `output_mode` integer and without defining the `output_sequence_length`. Use only 100 words as vocabulary (nothing good can be done with 100 words, but the purpose is to illustrate)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the size of the vocabulary and the max number of words in a sequence\n",
    "vocab_size = 100\n",
    "\n",
    "# Create a vectorization layer\n",
    "vectorize_layer = TextVectorization(\n",
    "    standardize = our_standardization,\n",
    "    max_tokens = vocab_size,\n",
    "    output_mode= 'int'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 4\n",
    "Adapt the vectorization layer to the text_data. Recall that this will build your vocabulary based on the provided text data. Specifically, the `vocab_size` most frequent tokens will make up your vocabulary. For illustration, print the first ten elements of your vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['', '[UNK]', 'the', 'a', 'and', 'of', 'to', 'is', 'in', 'it']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To create the vocabulary, we need to call adapt. The input is only the text\n",
    "vectorize_layer.adapt(train)\n",
    "vocab = vectorize_layer.get_vocabulary()\n",
    "vocab[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5\n",
    "Next we create *time series* of our text. Recall that, for language modelling, we need training data of the form $y=w_6$ and $x=[w_5,w_4,w_3,w_2,w_1]$, where the symbol $w$ is to represent one word. To achieve this, we supply the custom method `transform_text`. The core of the method is the built-in `timeseries_dataset_from_array` method provided by Keras.\n",
    "\n",
    "Make sure to examine the method to get an idea of the operations. We use it in the next exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_text(data, vectorize_layer, sequence_length):\n",
    "    #Each X[i] is an input of the text of sequence length and each y has exactly 1 observation, corresponding to the delay\n",
    "    delay = sequence_length # the target word is the word after the sequence\n",
    "    batch_size = 1\n",
    "    flag = True\n",
    "    # Generate data\n",
    "    for rev in data:\n",
    "        vec_rev = vectorize_layer(rev)\n",
    "        # Create time series dataset for each review\n",
    "        aux_dataset = tf.keras.preprocessing.timeseries_dataset_from_array(\n",
    "            data = vec_rev,\n",
    "            targets = vec_rev[delay:],\n",
    "            sequence_length=sequence_length,\n",
    "            shuffle=False,\n",
    "            batch_size=batch_size)\n",
    "        # Concatenate the time series\n",
    "        for input, target in aux_dataset:\n",
    "            if flag:\n",
    "                X = input\n",
    "                y = target\n",
    "                flag = False\n",
    "            else:     \n",
    "                X = tf.concat([X , input], 0)\n",
    "                y = tf.concat([y, target], 0)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 6\n",
    "Create the training and validation datasets using our custom function `transform_text()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train , y_train = transform_text(train, vectorize_layer, sequence_length)\n",
    "X_val, y_val = transform_text(val, vectorize_layer, sequence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec_train = vectorize_layer(val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4077"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([np.count_nonzero(vec_train[idx]) for idx in range(vec_train.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([np.count_nonzero(vec_train[idx]) for idx in range(vec_train.shape[0])]) - X_val.shape[0] == sequence_length * val.shape[0] #The difference seems to be **exactly** 5 * obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13820.0"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(100*691) / 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22273"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 7\n",
    "Check the frequency of each token (you can use `tf.unique_with_counts`). What's the problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.unique_with_counts(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feedforward NN\n",
    "### Exercise 8\n",
    "Fit a feedforward network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SimpleRNN\n",
    "### Exercise 9 \n",
    "Fit a NN with a `SimpleRNN` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. LSTM\n",
    "### Exercise 10\n",
    "Fit a NN with a `LSTM` layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Embedding + LSTM\n",
    "### Exercise 11\n",
    "Fit a NN with an `Embedding` and `LSTM` layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
